# Amir_Portfolio
# [Project 1: Evaluating Naive Bayes Classification Accuracy on the "PlayTennis" Dataset](https://github.com/Amirtegar/Amir_Portfolio/blob/main/Code/Project%201.ipynb) 
*Introduction*
*This case study aims to assess the performance of the Naive Bayes classification model in predicting the likelihood of playing tennis based on influencing factors. The "PlayTennis" dataset is utilized as the data source, comprising five main features, including temperature, humidity, wind speed, and others.
*Research Objective*
*The primary objective of this study is to evaluate the Naive Bayes model's ability to identify the probability of playing tennis or not based on observed features.
*Data Structure*
1. Outlook: Weather conditions during tennis play (Sunny, Overcast, Rain).
2. Temperature: Temperature during tennis play (Hot, Mild, Cool).
3. Humidity: Air humidity level (High, Normal).
4. Wind: Wind speed (Weak, Strong).
5. PlayTennis: Tennis playing decision (Yes, No).
*Accuracy Measurement*
*To measure model accuracy, standard classification evaluation metrics, including precision, recall, and F1-score, are used. However, the main focus is on the overall accuracy of the model in predicting tennis playing decisions.
## Overview Of The Confusion Matrix Nive Bayes
![](https://github.com/Amirtegar/Amir_Portfolio/blob/main/Image/Project%201.png)

# [Project 2: Mobile Phone Dataset Analysis](https://github.com/Amirtegar/Amir_Portfolio/blob/main/Code/Project%202.ipynb)
*Introduction*
*This study focuses on analyzing mobile phone data with a specific emphasis on measuring the percentage of mobile phones equipped with Bluetooth functionality. The dataset employed for this analysis is sourced from Kaggle and contains comprehensive information regarding mobile phone features.
*Objectives*
*The primary objectives of this study are:
1. To assess the prevalence of Bluetooth-enabled features across the dataset.
2. To apply both K-means clustering and linear regression for classification tasks.
*Dataset Overview*
*The "Mobile Phone Dataset" consists of diverse attributes related to mobile phones, with Bluetooth being a key feature of interest. Each data point represents a mobile phone entry with associated features.
## Overview Of K-Means Clustering
![](https://github.com/Amirtegar/Amir_Portfolio/blob/main/Image/project%202.png)

# [Project 3: Fire Incidents Analysis in Jakarta](https://github.com/Amirtegar/Amir_Portfolio/blob/main/Code/Data_Kebakaran.ipynb) 
*Introduction*
*This case study delves into the analysis of fire incidents within the Jakarta region, utilizing a comprehensive dataset specifically curated for this purpose. The dataset, herein referred to as "Fire Incidents Data," captures crucial information surrounding occurrences of fires in the Jakarta area.
*Objectives*
*The primary objectives of this study are as follows:
1. To assess the extent of damage caused by fire incidents, focusing on the count of damaged facilities and associated losses.
2. To identify the frequency of fire incidents in different regions and determine the areas most susceptible to such occurrences.
*Dataset Overview*
*The "Fire Incidents Data" is a rich dataset encompassing diverse attributes related to fire incidents, including but not limited to the location, extent of damage, and associated losses.
*Methodology*
*Damage Assessment:
1. Quantify the number of facilities damaged in each fire incident.
2. Evaluate the financial losses incurred due to these incidents.
*Regional Frequency Analysis:
1. Determine the frequency of fire incidents in various regions of Jakarta.
2. Identify hotspots and regions most prone to fire occurrences.
*Data Visualization:
1. Utilize various visualization techniques such as bar charts, pie charts, and other relevant visualizations to present key findings.
2. Visualize the distribution of damaged facilities, financial losses, and regional fire incident frequencies.
## An overview of the region with the highest number of affected casualties
![](https://github.com/Amirtegar/Amir_Portfolio/blob/main/Image/project%203.png)

# [Project 4: Abalone Dataset Analysis](https://github.com/Amirtegar/Amir_Portfolio/blob/main/Code/Project%204.ipynb) 
*Introduction*
*This case study revolves around the exploration and analysis of the "Abalone" dataset. The primary purpose of this study is to serve as a practical exercise in data cleaning, encoding, and the application of various algorithms. The ultimate objective is to rigorously test the accuracy of each algorithm applied in the context of the dataset.
*Dataset Overview*
*The "Abalone" dataset encapsulates information related to abalones, a type of marine mollusk. The dataset includes diverse attributes, providing a comprehensive foundation for testing and validating various data science techniques.
*Objectives*
1. Data Cleaning:
*Execute meticulous data cleaning processes to ensure the dataset's integrity and reliability.
2. Encoding:
*Implement encoding techniques for categorical variables, facilitating the preparation of data for algorithmic application.
3. Algorithm Application:
*Apply a range of algorithms to the encoded dataset to evaluate their respective performances.
*Explore algorithms suitable for the specific characteristics of the "Abalone" dataset.
3. Accuracy Assessment:
*Rigorously test and assess the accuracy of each applied algorithm.
*Compare and contrast the performance of algorithms to identify the most effective one for the given dataset.

# [Project 5: Earthquake Database Analysis](https://github.com/Amirtegar/Amir_Portfolio/blob/main/Code/Project%205.ipynb)
*Introduction*
*This case study delves into the analysis of the "earthquake-database" dataset, accessible on Kaggle.com. The primary objective is to visualize earthquake data, unraveling insights into seismic events and their associated features.
*Dataset Overview*
*The "earthquake-database" dataset provides a comprehensive repository of information related to earthquakes. This dataset encompasses various attributes that offer a nuanced understanding of seismic occurrences.
*Objectives*
1. Feature Exploration:
*Initiate the study by comprehensively understanding the features present in the earthquake dataset.
2. Target Selection:
*Identify and select target features that will be the focus of subsequent visualizations.
3. Missing Value Check:
*Perform a meticulous examination of missing values within the dataset to ensure data integrity.
4. Correlation Determination:
*Analyze and determine the correlation between different features to unveil potential patterns and relationships.
## Overview Of The Earthquake Visualization
![](https://github.com/Amirtegar/Amir_Portfolio/blob/main/Image/project%205.png)

# [Project 6: Titanic3 Dataset Analysis](https://github.com/Amirtegar/Amir_Portfolio/blob/main/Code/Project%206.ipynb) 
*Introduction*
*This case study centers around the dataset "titanic3" with a primary objective of performing thorough data cleaning. The study commences with a descriptive analytics phase, followed by preprocessing steps involving the identification and handling of missing values. The final steps include normalization and the application of logarithmic classification.
*Dataset Overview*
*The "titanic3" dataset encapsulates information related to passengers aboard the Titanic, providing a comprehensive foundation for data analysis.
*Objectives*
*The core objectives of this case study are as follows:
1. Data Cleaning:
*Execute meticulous data cleaning procedures to enhance the integrity and quality of the dataset.
2. Descriptive Analytics:
*Begin the analysis with a descriptive analytics phase to gain insights into the dataset's characteristics.
3. Preprocessing with Missing Value Check:
*Perform preprocessing tasks, including the identification and handling of missing values.
*Implement strategies to ensure completeness and reliability in the dataset.
4. Normalization and Logarithmic Classification:
*Apply normalization techniques to standardize numerical features.
*Implement logarithmic classification for specific variables, as deemed appropriate.
# [Project 7: Public E-commerce Dataset Analysis](https://github.com/Amirtegar/Amir_Portfolio/blob/main/Code/Analisis_Data_E_Commerce_Public_Dataset.ipynb)) 
*Introduction*
*This project centers on the analysis of a public e-commerce dataset with the primary aim of extracting insights into sales trends and determining the most dominant states in terms of order placement. The analysis encompasses data cleaning, exploratory data analysis (EDA), and the visualization of key findings.
*Dataset Overview*
*The e-commerce dataset under scrutiny contains diverse attributes related to sales transactions, offering a comprehensive foundation for in-depth analysis.
*Objectives
1. Sales Trend Analysis:
*Uncover and interpret trends within the sales data to gain insights into the dynamics of purchasing behavior.
2. Dominant State Identification:
*Identify and analyze the states that exhibit dominance in terms of the frequency of order placement.
3. Data Cleaning:
*Execute meticulous data cleaning processes to ensure the integrity and reliability of the dataset.
4. Exploratory Data Analysis (EDA):
*Perform exploratory data analysis to understand the distribution, relationships, and key statistical measures within the dataset.
5. Data Visualization:
*Create visual representations of the analyzed data to facilitate the communication of key findings in a clear and interpretable manner.
# [Project 8: Image Classification Project with Machine Learning](https://github.com/Amirtegar/Amir_Portfolio/blob/main/Code/Proyek_Akhir_Klasifikasi_Gambar.ipynb) 
*Overview*
*This project involves image classification using machine learning, specifically implementing a convolutional neural network (CNN) with the following key specifications:
*Dataset Preparation*
1. Train-Validation Split:
*The dataset is divided into training and validation sets.
2. Augmentation:
*Image augmentation techniques are implemented to enhance the diversity of the training dataset.
3. Image Data Generator:
*The ImageDataGenerator class is employed to handle data loading and augmentation during model training.
4. Sequential Model:
*The model architecture is designed using the Sequential API in TensorFlow/Keras.
5. Training Time Limit:
*Model training is constrained to a maximum duration of 30 minutes to optimize efficiency.
6. Accuracy Threshold:
*The model is trained to achieve a minimum accuracy of 85% on the validation set.
7. Prediction on Uploaded Images:
The trained model is capable of making predictions on images uploaded to Colab.
